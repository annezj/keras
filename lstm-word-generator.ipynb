{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-level text generator using an embedded layer to learn\n",
    "word representation and an LSTM RNN to predict words.\n",
    "example from https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               A long time ago, in a galaxy far, far, away...\n",
      "\n",
      "               A vast sea of stars serves as the backdrop for the main title. \n",
      "               War drums echo through the heavens as a ro\n"
     ]
    }
   ],
   "source": [
    "# load the text\n",
    "def load_doc(filename):\n",
    "    file=open(filename, 'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "in_filename = '/Users/annejones/Documents/nn/datasets/anh-script.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# replace '--' with a space ' '\n",
    "\tdoc = doc.replace('--', ' ')\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'long', 'time', 'ago', 'in', 'a', 'galaxy', 'far', 'far', 'away', 'a', 'vast', 'sea', 'of', 'stars', 'serves', 'as', 'the', 'backdrop', 'for', 'the', 'main', 'title', 'war', 'drums', 'echo', 'through', 'the', 'heavens', 'as', 'a', 'rollup', 'slowly', 'crawls', 'into', 'infinity', 'it', 'is', 'a', 'period', 'of', 'civil', 'war', 'rebel', 'spaceships', 'striking', 'from', 'a', 'hidden', 'base', 'have', 'won', 'their', 'first', 'victory', 'against', 'the', 'evil', 'galactic', 'empire', 'during', 'the', 'battle', 'rebel', 'spies', 'managed', 'to', 'steal', 'secret', 'plans', 'to', 'the', 'empires', 'ultimate', 'weapon', 'the', 'death', 'star', 'an', 'armored', 'space', 'station', 'with', 'enough', 'power', 'to', 'destroy', 'an', 'entire', 'planet', 'pursued', 'by', 'the', 'empires', 'sinister', 'agents', 'princess', 'leia', 'races', 'home', 'aboard', 'her', 'starship', 'custodian', 'of', 'the', 'stolen', 'plans', 'that', 'can', 'save', 'her', 'people', 'and', 'restore', 'freedom', 'to', 'the', 'galaxy', 'the', 'awesome', 'yellow', 'planet', 'of', 'tatooine', 'emerges', 'from', 'a', 'total', 'eclipse', 'her', 'two', 'moons', 'glowing', 'against', 'the', 'darkness', 'a', 'tiny', 'silver', 'spacecraft', 'a', 'rebel', 'blockade', 'runner', 'firing', 'lasers', 'from', 'the', 'back', 'of', 'the', 'ship', 'races', 'through', 'space', 'it', 'is', 'pursed', 'by', 'a', 'giant', 'imperial', 'stardestroyer', 'hundreds', 'of', 'deadly', 'laserbolts', 'streak', 'from', 'the', 'imperial', 'stardestroyer', 'causing', 'the', 'main', 'solar', 'fin', 'of', 'the', 'rebel', 'craft', 'to', 'disintegrate', 'int', 'rebel', 'blockade', 'runner', 'main', 'passageway', 'an', 'explosion', 'rocks', 'the', 'ship', 'as', 'two', 'robots', 'artoodetoo', 'and']\n",
      "Total Tokens: 32695\n",
      "Unique Tokens: 3923\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 32644\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "    # save sequences to file\n",
    "out_filename = '/Users/annejones/Documents/nn/datasets/pp_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and split based on new lines\n",
    "in_filename = '/Users/annejones/Documents/nn/datasets/pp_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output and one\n",
    "# hot encode the output\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from numpy import array\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 50)            196200    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3924)              396324    \n",
      "=================================================================\n",
      "Total params: 743,424\n",
      "Trainable params: 743,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32644/32644 [==============================] - 68s 2ms/step - loss: 6.5968 - acc: 0.0644\n",
      "Epoch 2/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 6.3741 - acc: 0.0644\n",
      "Epoch 3/100\n",
      "32644/32644 [==============================] - 69s 2ms/step - loss: 6.2671 - acc: 0.0677\n",
      "Epoch 4/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 6.1526 - acc: 0.0678\n",
      "Epoch 5/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 6.0684 - acc: 0.0688\n",
      "Epoch 6/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 6.0041 - acc: 0.0685\n",
      "Epoch 7/100\n",
      "32644/32644 [==============================] - 85s 3ms/step - loss: 5.9481 - acc: 0.0695\n",
      "Epoch 8/100\n",
      "32644/32644 [==============================] - 71s 2ms/step - loss: 5.9010 - acc: 0.0725\n",
      "Epoch 9/100\n",
      "32644/32644 [==============================] - 71s 2ms/step - loss: 5.8563 - acc: 0.0782\n",
      "Epoch 10/100\n",
      "32644/32644 [==============================] - 79s 2ms/step - loss: 5.8146 - acc: 0.0799\n",
      "Epoch 11/100\n",
      "32644/32644 [==============================] - 72s 2ms/step - loss: 5.7792 - acc: 0.0834\n",
      "Epoch 12/100\n",
      "32644/32644 [==============================] - 69s 2ms/step - loss: 5.7420 - acc: 0.0869\n",
      "Epoch 13/100\n",
      "32644/32644 [==============================] - 71s 2ms/step - loss: 5.7062 - acc: 0.0891\n",
      "Epoch 14/100\n",
      "32644/32644 [==============================] - 69s 2ms/step - loss: 5.6723 - acc: 0.0908\n",
      "Epoch 15/100\n",
      "32644/32644 [==============================] - 69s 2ms/step - loss: 5.6386 - acc: 0.0923\n",
      "Epoch 16/100\n",
      "32644/32644 [==============================] - 69s 2ms/step - loss: 5.6011 - acc: 0.0958\n",
      "Epoch 17/100\n",
      "32644/32644 [==============================] - 70s 2ms/step - loss: 5.5634 - acc: 0.0998\n",
      "Epoch 18/100\n",
      "32644/32644 [==============================] - 77s 2ms/step - loss: 5.5259 - acc: 0.1014\n",
      "Epoch 19/100\n",
      "32644/32644 [==============================] - 69s 2ms/step - loss: 5.4884 - acc: 0.1046\n",
      "Epoch 20/100\n",
      "32644/32644 [==============================] - 69s 2ms/step - loss: 5.4515 - acc: 0.1070\n",
      "Epoch 21/100\n",
      "32644/32644 [==============================] - 68s 2ms/step - loss: 5.4139 - acc: 0.1098\n",
      "Epoch 22/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 5.3721 - acc: 0.1142\n",
      "Epoch 23/100\n",
      "32644/32644 [==============================] - 72s 2ms/step - loss: 5.3391 - acc: 0.1150\n",
      "Epoch 24/100\n",
      "32644/32644 [==============================] - 71s 2ms/step - loss: 5.3046 - acc: 0.1191\n",
      "Epoch 25/100\n",
      "32644/32644 [==============================] - 69s 2ms/step - loss: 5.2687 - acc: 0.1214\n",
      "Epoch 26/100\n",
      "32644/32644 [==============================] - 68s 2ms/step - loss: 5.2361 - acc: 0.1224\n",
      "Epoch 27/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 5.2024 - acc: 0.1259\n",
      "Epoch 28/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 5.1744 - acc: 0.1261\n",
      "Epoch 29/100\n",
      "32644/32644 [==============================] - 618s 19ms/step - loss: 5.1444 - acc: 0.1292\n",
      "Epoch 30/100\n",
      "32644/32644 [==============================] - 70s 2ms/step - loss: 5.1194 - acc: 0.1299\n",
      "Epoch 31/100\n",
      "32644/32644 [==============================] - 71s 2ms/step - loss: 5.0978 - acc: 0.1298\n",
      "Epoch 32/100\n",
      "32644/32644 [==============================] - 84s 3ms/step - loss: 5.0707 - acc: 0.1309\n",
      "Epoch 33/100\n",
      "32644/32644 [==============================] - 87s 3ms/step - loss: 5.0456 - acc: 0.1335\n",
      "Epoch 34/100\n",
      "32644/32644 [==============================] - 82s 3ms/step - loss: 5.0171 - acc: 0.1358\n",
      "Epoch 35/100\n",
      "32644/32644 [==============================] - 76s 2ms/step - loss: 4.9928 - acc: 0.1376\n",
      "Epoch 36/100\n",
      "32644/32644 [==============================] - 92s 3ms/step - loss: 4.9683 - acc: 0.1400\n",
      "Epoch 37/100\n",
      "32644/32644 [==============================] - 73s 2ms/step - loss: 4.9503 - acc: 0.1416\n",
      "Epoch 38/100\n",
      "32644/32644 [==============================] - 69s 2ms/step - loss: 4.9230 - acc: 0.1437\n",
      "Epoch 39/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.8985 - acc: 0.1459\n",
      "Epoch 40/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.8767 - acc: 0.1473\n",
      "Epoch 41/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.8537 - acc: 0.1506\n",
      "Epoch 42/100\n",
      "32644/32644 [==============================] - 71s 2ms/step - loss: 4.8327 - acc: 0.1513\n",
      "Epoch 43/100\n",
      "32644/32644 [==============================] - 72s 2ms/step - loss: 4.8070 - acc: 0.1536\n",
      "Epoch 44/100\n",
      "32644/32644 [==============================] - 69s 2ms/step - loss: 4.7844 - acc: 0.1550\n",
      "Epoch 45/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.7664 - acc: 0.1568\n",
      "Epoch 46/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.7486 - acc: 0.1576\n",
      "Epoch 47/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.7179 - acc: 0.1600\n",
      "Epoch 48/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.7021 - acc: 0.1613\n",
      "Epoch 49/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.6749 - acc: 0.1640\n",
      "Epoch 50/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.6627 - acc: 0.1646\n",
      "Epoch 51/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.6400 - acc: 0.1663\n",
      "Epoch 52/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.6258 - acc: 0.1663\n",
      "Epoch 53/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.6011 - acc: 0.1684\n",
      "Epoch 54/100\n",
      "32644/32644 [==============================] - 68s 2ms/step - loss: 4.5795 - acc: 0.1703\n",
      "Epoch 55/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.5571 - acc: 0.1716\n",
      "Epoch 56/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.5387 - acc: 0.1731\n",
      "Epoch 57/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.5212 - acc: 0.1751\n",
      "Epoch 58/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.4986 - acc: 0.1771\n",
      "Epoch 59/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.4798 - acc: 0.1790\n",
      "Epoch 60/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.4668 - acc: 0.1791\n",
      "Epoch 61/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.4446 - acc: 0.1815\n",
      "Epoch 62/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.4324 - acc: 0.1820\n",
      "Epoch 63/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.4121 - acc: 0.1823\n",
      "Epoch 64/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.3967 - acc: 0.1838\n",
      "Epoch 65/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.3817 - acc: 0.1853\n",
      "Epoch 66/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.3557 - acc: 0.1877\n",
      "Epoch 67/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.3378 - acc: 0.1892\n",
      "Epoch 68/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.3219 - acc: 0.1906\n",
      "Epoch 69/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.3134 - acc: 0.1906\n",
      "Epoch 70/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.2943 - acc: 0.1902\n",
      "Epoch 71/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.2776 - acc: 0.1944\n",
      "Epoch 72/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.2581 - acc: 0.1934\n",
      "Epoch 73/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.2432 - acc: 0.1965\n",
      "Epoch 74/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.2273 - acc: 0.1985\n",
      "Epoch 75/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.2181 - acc: 0.1987\n",
      "Epoch 76/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.2001 - acc: 0.1996\n",
      "Epoch 77/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.1890 - acc: 0.2010\n",
      "Epoch 78/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.1726 - acc: 0.2028\n",
      "Epoch 79/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.1560 - acc: 0.2044\n",
      "Epoch 80/100\n",
      "32644/32644 [==============================] - 67s 2ms/step - loss: 4.1455 - acc: 0.2063\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32644/32644 [==============================] - 66s 2ms/step - loss: 4.1387 - acc: 0.2074\n",
      "Epoch 82/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 4.1192 - acc: 0.2080\n",
      "Epoch 83/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 4.1030 - acc: 0.2097\n",
      "Epoch 84/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 4.0932 - acc: 0.2102\n",
      "Epoch 85/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 4.0775 - acc: 0.2116\n",
      "Epoch 86/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 4.0612 - acc: 0.2143\n",
      "Epoch 87/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 4.0527 - acc: 0.2133\n",
      "Epoch 88/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 4.0327 - acc: 0.2166\n",
      "Epoch 89/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 4.0197 - acc: 0.2185\n",
      "Epoch 90/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 4.0086 - acc: 0.2198\n",
      "Epoch 91/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 3.9987 - acc: 0.2202\n",
      "Epoch 92/100\n",
      "32644/32644 [==============================] - 66s 2ms/step - loss: 3.9865 - acc: 0.2209\n",
      "Epoch 93/100\n",
      "32644/32644 [==============================] - 70s 2ms/step - loss: 3.9679 - acc: 0.2234\n",
      "Epoch 94/100\n",
      "32644/32644 [==============================] - 73s 2ms/step - loss: 3.9594 - acc: 0.2248\n",
      "Epoch 95/100\n",
      "32644/32644 [==============================] - 73s 2ms/step - loss: 3.9474 - acc: 0.2252\n",
      "Epoch 96/100\n",
      "32644/32644 [==============================] - 74s 2ms/step - loss: 3.9293 - acc: 0.2274\n",
      "Epoch 97/100\n",
      "32644/32644 [==============================] - 73s 2ms/step - loss: 3.9204 - acc: 0.2289\n",
      "Epoch 98/100\n",
      "32644/32644 [==============================] - 75s 2ms/step - loss: 3.9118 - acc: 0.2284\n",
      "Epoch 99/100\n",
      "32644/32644 [==============================] - 79s 2ms/step - loss: 3.8963 - acc: 0.2293\n",
      "Epoch 100/100\n",
      "32644/32644 [==============================] - 72s 2ms/step - loss: 3.8774 - acc: 0.2317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x116cb3128>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to generate text\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from pickle import load\n",
    "from random import randint\n",
    "in_filename = '/Users/annejones/Documents/nn/datasets/pp_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split())-1\n",
    "model = load_model('model.h5')\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few feet of lukes face luke doesnt move and the ball backs off it slowly moves behind the boy then makes another quick lunge this time emitting a blood red laser beam as it attacks it hits luke in the leg causing him to tumble over han lets loose with a...\n",
      "small small pistol and starts to the old jedi luke is a few moments luke i think you think he\n"
     ]
    }
   ],
   "source": [
    "# get seed text\n",
    "seed_text = lines[randint(0, len(lines))]\n",
    "generated = generate_seq(model, tokenizer, \n",
    "                         seq_length, seed_text, 20)\n",
    "print(seed_text + '...')\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-env]",
   "language": "python",
   "name": "conda-env-test-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
